{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50069e64",
   "metadata": {},
   "source": [
    "# CV6 Inference (Single Model)\n",
    "\n",
    "- Full-frame rectangular input\n",
    "- CLS + patch mean pooling\n",
    "- SSF adapters enabled\n",
    "- Zero-inflated clover + physics constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ca408",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import timm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def flush() -> None:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25279b93",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = Path(\"/kaggle/input/csiro-biomass\")\n",
    "    WEIGHTS_PATH = Path(\"/kaggle/input/pretrained-weights-biomass/dinov3_large/dinov3_large\")\n",
    "    # Update this to your uploaded model dataset\n",
    "    MODEL_PATH = Path(\"/kaggle/input/csiro-cv6-models/cv6_model_best.pth\")\n",
    "\n",
    "    model_name = \"vit_large_patch16_dinov3_qkvb.lvd1689m\"\n",
    "    img_size = (336, 784)  # (H, W)\n",
    "\n",
    "    head_dim = 256\n",
    "    head_layers = 2\n",
    "    dropout = 0.2\n",
    "\n",
    "    freeze_backbone = True\n",
    "    use_ssf = True\n",
    "    ssf_per_block = True\n",
    "\n",
    "    batch_size = 16\n",
    "    num_workers = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    use_tta = False  # keep false for deterministic single-pass\n",
    "\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(42)\n",
    "\n",
    "TARGET_ORDER = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af9fc6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "def get_test_transform(cfg: CFG) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.Resize(cfg.img_size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037c4e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, cfg: CFG, transform: T.Compose):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cfg = cfg\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(self.cfg.DATA_PATH / row['image_path']).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        assert img.ndim == 3 and img.shape[0] == 3, f\"Unexpected image shape: {img.shape}\"\n",
    "        assert img.shape[1] % 16 == 0 and img.shape[2] % 16 == 0, (\n",
    "            f\"Image H/W must be divisible by 16, got {img.shape[1:]}\"\n",
    "        )\n",
    "        return img, row['sample_id_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a82d84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SSFAdapter(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1, 1, dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self.scale + self.shift\n",
    "\n",
    "\n",
    "class DINOv3Backbone(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\"\n",
    "        )\n",
    "\n",
    "        weights_file = cfg.WEIGHTS_PATH / \"dinov3_vitl16_qkvb.pth\"\n",
    "        if weights_file.exists():\n",
    "            state = torch.load(weights_file, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(state, strict=False)\n",
    "            print(f\"✓ Loaded backbone weights from {weights_file}\")\n",
    "        else:\n",
    "            print(\"⚠️ Backbone weights not found; using random init\")\n",
    "\n",
    "        self.num_features = self.backbone.num_features\n",
    "\n",
    "        if cfg.freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.ssf_per_block = cfg.use_ssf and cfg.ssf_per_block\n",
    "        self.use_ssf = cfg.use_ssf\n",
    "        self.ssf_blocks = None\n",
    "        self._hooks = []\n",
    "\n",
    "        if self.use_ssf:\n",
    "            if self.ssf_per_block:\n",
    "                self._init_ssf_per_block()\n",
    "            else:\n",
    "                self.ssf_out = SSFAdapter(self.num_features)\n",
    "\n",
    "    def _init_ssf_per_block(self) -> None:\n",
    "        assert hasattr(self.backbone, 'blocks'), \"Backbone has no blocks attribute\"\n",
    "        self.ssf_blocks = nn.ModuleList([\n",
    "            SSFAdapter(self.num_features) for _ in range(len(self.backbone.blocks))\n",
    "        ])\n",
    "\n",
    "        def make_hook(i: int):\n",
    "            def hook(_module, _input, output):\n",
    "                return self.ssf_blocks[i](output)\n",
    "            return hook\n",
    "\n",
    "        for i, blk in enumerate(self.backbone.blocks):\n",
    "            self._hooks.append(blk.register_forward_hook(make_hook(i)))\n",
    "\n",
    "    def forward_tokens(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = self.backbone.forward_features(x)\n",
    "        if isinstance(tokens, (tuple, list)):\n",
    "            tokens = tokens[0]\n",
    "        if tokens.ndim == 2:\n",
    "            tokens = tokens.unsqueeze(1)\n",
    "        if self.use_ssf and (not self.ssf_per_block):\n",
    "            tokens = self.ssf_out(tokens)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class ZeroInflatedHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 128, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        p_pos = self.classifier(x)\n",
    "        amount = self.regressor(x)\n",
    "        pred = p_pos * amount\n",
    "        return p_pos, amount, pred\n",
    "\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur = in_dim\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(cur, hidden_dim))\n",
    "            if i < num_layers - 1:\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            cur = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class CV6Model(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.backbone = DINOv3Backbone(cfg)\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.out_dim = feat_dim * 2\n",
    "\n",
    "        self.green_head = MLPHead(self.out_dim, cfg.head_dim, cfg.head_layers, cfg.dropout)\n",
    "        self.dead_head = MLPHead(self.out_dim, cfg.head_dim, cfg.head_layers, cfg.dropout)\n",
    "        self.clover_head = ZeroInflatedHead(self.out_dim, hidden_dim=cfg.head_dim, dropout=cfg.dropout)\n",
    "\n",
    "        self.softplus = nn.Softplus(beta=1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        tokens = self.backbone.forward_tokens(x)\n",
    "        B, N, C = tokens.shape\n",
    "        cls = tokens[:, 0]\n",
    "        if N > 1:\n",
    "            patch_mean = tokens[:, 1:].mean(dim=1)\n",
    "        else:\n",
    "            patch_mean = cls\n",
    "\n",
    "        feat = torch.cat([cls, patch_mean], dim=1)\n",
    "\n",
    "        green = self.softplus(self.green_head(feat))\n",
    "        dead = self.softplus(self.dead_head(feat))\n",
    "        _, _, clover = self.clover_head(feat)\n",
    "\n",
    "        gdm = green + clover\n",
    "        total = gdm + dead\n",
    "\n",
    "        return torch.cat([green, dead, clover, gdm, total], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model: nn.Module, loader: DataLoader, device: str) -> Tuple[np.ndarray, list]:\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ids = []\n",
    "    for images, sample_ids in tqdm(loader, desc=\"Predict\"):\n",
    "        images = images.to(device)\n",
    "        out = model(images)\n",
    "        preds.append(out.cpu().numpy())\n",
    "        ids.extend(sample_ids)\n",
    "    return np.concatenate(preds, axis=0), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dda194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "test_df = pd.read_csv(cfg.DATA_PATH / \"test.csv\")\n",
    "\n",
    "test_df['sample_id_prefix'] = test_df['sample_id'].str.split('__').str[0]\n",
    "# Unique images only\n",
    "test_wide = test_df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "print(f\"Test images: {len(test_wide)}\")\n",
    "\n",
    "transform = get_test_transform(cfg)\n",
    "\n",
    "test_ds = TestDataset(test_wide, cfg, transform)\n",
    "loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model = CV6Model(cfg).to(cfg.device)\n",
    "state = torch.load(cfg.MODEL_PATH, map_location=cfg.device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "print(f\"✓ Loaded model: {cfg.MODEL_PATH}\")\n",
    "\n",
    "preds, sample_ids = predict(model, loader, cfg.device)\n",
    "print(f\"Pred shape: {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build submission\n",
    "\n",
    "pred_df = pd.DataFrame(preds, columns=TARGET_ORDER)\n",
    "pred_df['sample_id_prefix'] = sample_ids\n",
    "\n",
    "sub_df = pred_df.melt(\n",
    "    id_vars=['sample_id_prefix'],\n",
    "    value_vars=TARGET_ORDER,\n",
    "    var_name='target_name',\n",
    "    value_name='target'\n",
    ")\n",
    "sub_df['sample_id'] = sub_df['sample_id_prefix'] + '__' + sub_df['target_name']\n",
    "\n",
    "submission = sub_df[['sample_id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Saved submission.csv with {len(submission)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "sample_sub = pd.read_csv(cfg.DATA_PATH / \"sample_submission.csv\")\n",
    "assert len(submission) == len(sample_sub)\n",
    "print(\"✓ Format verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294914a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "del model\n",
    "flush()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
